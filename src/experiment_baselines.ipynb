{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85f9b686-4dee-4846-b006-9d3b39292662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Noto Sans CJK JP'\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.scaler import STMatrixStandardScaler\n",
    "from utils.helper import format_stmatrix, train_test_split, fix_seed, seed_worker\n",
    "from encdec.dataset import STDataset\n",
    "from encdec.trainer import Trainer\n",
    "from logger import Logger\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a66af52-fa9b-44a1-8e3d-7c1ab733e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f0112-9df1-4430-a141-c4b94a5a4962",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 前処理してデータセットを作成\n",
    "- 渋滞量 -> フラグに変換\n",
    "- 方向 -> 0/1に変換\n",
    "    - 上り: 0, 下り: 1\n",
    "- 四半期を数値化\n",
    "- 使用しないカラムを落とす\n",
    "    - 天気 + `index`, `data`, `road_code`, `jam_type`\n",
    "- 速度の欠損を埋める\n",
    "- OCC -> [0, 1]に変換\n",
    "- 型変換\n",
    "    - float64 -> float32\n",
    "    - 区間の名前, コード, 県コード, 0/1系, カレンダーデータをcategoryデータに\n",
    "    - degreeをint32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b47c5-ece1-4061-b622-f0c26c9de6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 道路名\n",
    "# TARGET_ROAD='tateyama'\n",
    "TARGET_ROAD='kannetsu'\n",
    "\n",
    "# 交通量\n",
    "PROCESSED_DATA_DIR = '../Input_processed_data'\n",
    "TRAFFIC_DIR = f'{PROCESSED_DATA_DIR}/traffic'\n",
    "TRAFFIC_CSV = f'{TRAFFIC_DIR}/{TARGET_ROAD}_20220621all-merged_filled_1h.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58e808-e64e-4d7d-836d-04fd532a1054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_types = {'start_code': str, 'end_code': str, 'road_code': str, 'jam_type': str,}\n",
    "\n",
    "df = pd.read_csv(TRAFFIC_CSV, parse_dates=True, index_col='datetime', dtype=col_types).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898041e-0cc7-420f-9363-73a3edc61e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolate(df, col):\n",
    "    '''\n",
    "    dfのcolカラム内の欠損を区間ごとに線形補間する\n",
    "    '''\n",
    "    f = lambda g: g.interpolate(method='linear', axis=0)\n",
    "    \n",
    "    df.sort_values('datetime', inplace=True)\n",
    "    df[col] = df.groupby(['start_code', 'end_code'])[col].apply(f)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df):\n",
    "    # 「年」情報を入れる\n",
    "    df['year'] = df['datetime'].dt.year\n",
    "    # 渋滞フラグ 0/1\n",
    "    df['jam_quantity'] = np.where(df['speed'] < 40, 1, 0)\n",
    "    # 方向を数値化\n",
    "    direction_map = {'上り': 0, '下り': 1}\n",
    "    df['direction'] = df['direction'].map(direction_map)\n",
    "    # 四半期を数値化\n",
    "    df['quarter'] = df['quarter'].str[-1]\n",
    "    \n",
    "    # object型のカラム, いらないカラムを落とす\n",
    "    drop_cols = [\n",
    "        'index', 'date', 'road_code', 'pressure', 'rainfall', \n",
    "        'temperature', 'humidity', 'wind_speed', 'daylight_hours', \n",
    "        'snowfall', 'deepest_snowfall', 'weather_description', 'jam_type'\n",
    "    ]\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # 速度の欠損を埋める\n",
    "    df = linear_interpolate(df, 'speed')\n",
    "    # OCCを[0,1]に変換\n",
    "    df['OCC'] = df['OCC'] / 100.0\n",
    "    \n",
    "    # 型変換\n",
    "    f64_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df.loc[:, f64_cols] = df.loc[:, f64_cols].astype(np.float32)\n",
    "    i64_cols = df.select_dtypes(include=[int]).columns\n",
    "    df.loc[:, i64_cols] = df.loc[:, i64_cols].astype(np.int32)\n",
    "    \n",
    "    type_map = {\n",
    "        'start_name': 'category',\n",
    "        'end_name': 'category',\n",
    "        'start_code': 'category',\n",
    "        'end_code': 'category',\n",
    "        'start_pref_code': 'category',\n",
    "        'end_pref_code': 'category',\n",
    "        'direction': 'category',\n",
    "        'month': 'category',\n",
    "        'day': 'category',\n",
    "        'dayofweek': 'category',\n",
    "        'is_holiday': 'category',\n",
    "        'hour': 'category',\n",
    "        'quarter': 'category',\n",
    "        'jam_quantity': 'category',\n",
    "        'start_degree': np.int32,\n",
    "        'end_degree': np.int32,\n",
    "        'degree_sum': np.int32,\n",
    "    }\n",
    "    df = df.astype(type_map)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d5e8e-66a4-4c63-a76c-984f49cdec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, start_date, end_date, pkl_name):\n",
    "    tmp = df.loc[(df['datetime'] >= pd.Timestamp(start_date)) & (df['datetime'] < pd.Timestamp(end_date))]\n",
    "    # tmp.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    tmp = preprocess(tmp.copy())\n",
    "    tmp.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    tmp.to_pickle(pkl_name)\n",
    "\n",
    "# whole dataset\n",
    "start_date = '2021/4/2'\n",
    "end_date = '2022/6/1'\n",
    "pkl_name = './datasets_1h/kannetsu_210402-220531.pkl'\n",
    "\n",
    "create_dataset(df, start_date, end_date, pkl_name)\n",
    "\n",
    "# mini dataset\n",
    "start_date = '2021/4/2'\n",
    "end_date = '2021/6/1'\n",
    "pkl_name = './datasets_1h/kannetsu_210402-210531.pkl'\n",
    "\n",
    "create_dataset(df, start_date, end_date, pkl_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c7f12-2979-4952-8165-d5c01122d1f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## データセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840688e-38f3-4355-bd7e-e1655e966cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini\n",
    "df_test = pd.read_pickle('./datasets_1h/kannetsu_210402-210531.pkl')\n",
    "# whole\n",
    "df_all = pd.read_pickle('./datasets_1h/kannetsu_210402-220531.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5f703-fc2c-4d76-9bb8-61bc9e565947",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_table = pd.read_pickle(f'{config.TABLES_DIR}/datetime_table.pkl')\n",
    "sec_table = pd.read_pickle(f'{config.TABLES_DIR}/section_table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9502253-a633-48b8-bc79-f90b5cf43d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e4d66-8ae9-4d1a-a770-d55c4c82dfce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 時間, 区間にembedding用のIDを割り振る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e91cfa6-79bd-4901-b38d-7e436704000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時間情報を管理するためのテーブルを作成 (month x hour x dayofweeks x is_holidays)\n",
    "# months = range(1, 12+1)\n",
    "# hours = range(24)\n",
    "# dayofweeks = range(1, 7+1)\n",
    "# is_holidays = (0, 1)\n",
    "\n",
    "# dt_table = pd.DataFrame(itertools.product(months, hours, dayofweeks, is_holidays), columns=['month', 'hour', 'dayofweek', 'is_holiday'], dtype='category')\n",
    "# dt_table = dt_table.query('dayofweek not in (6, 7) | is_holiday != 0').reset_index(drop=True)\n",
    "# dt_table = dt_table.reset_index().set_index(['month', 'hour', 'dayofweek', 'is_holiday']).astype('category')\n",
    "\n",
    "# dt_table.to_pickle('./datasets/datetime_table.pkl')\n",
    "\n",
    "# dt_table = pd.read_pickle('./datasets/datetime_table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48291632-a12e-4e3c-a8f5-9ba4e8f91aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時間情報を管理するためのテーブルを作成 (hour x dayofweeks x is_holidays)\n",
    "# hours = range(24)\n",
    "# dayofweeks = range(1, 7+1)\n",
    "# is_holidays = (0, 1)\n",
    "\n",
    "# dt_table = pd.DataFrame(itertools.product(hours, dayofweeks, is_holidays), columns=['hour', 'dayofweek', 'is_holiday'], dtype='category')\n",
    "# dt_table = dt_table.query('dayofweek not in (6, 7) | is_holiday != 0').reset_index(drop=True)\n",
    "# dt_table = dt_table.reset_index().set_index(['hour', 'dayofweek', 'is_holiday']).astype('category')\n",
    "\n",
    "# dt_table.to_pickle('./datasets/mini_datetime_table.pkl')\n",
    "\n",
    "# dt_table = pd.read_pickle('./datasets/mini_datetime_table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b54d50-3eba-4d3e-93ea-768d54c39097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 区間情報を管理するためのテーブルを作成\n",
    "# sec_table = df_test[['start_name', 'end_name', 'direction', 'KP']].drop_duplicates()\n",
    "# 区間順にソート\n",
    "# sort_f = lambda g: g.sort_values('KP', ascending=(g.name == 1))\n",
    "# sec_table = sec_table.groupby('direction').apply(sort_f).reset_index(drop=True)\n",
    "\n",
    "# sec_table.to_pickle('./datasets/section_table.pkl')\n",
    "# sec_table.head(3)\n",
    "\n",
    "# sec_table = pd.read_pickle('./datasets/section_table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac6bbd-5e5d-4daa-b997-400b4b7672f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime2id(df, dt_table):\n",
    "    time_col = ['hour', 'dayofweek', 'is_holiday']\n",
    "    f = lambda g: g.assign(datetime_id=dt_table.loc[g.name, 'index'])\n",
    "    df = df.groupby(time_col).apply(f)\n",
    "    df['datetime_id'] = df['datetime_id'].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def section2id(df, sec_table):\n",
    "    f = lambda g: g.assign(section_id=sec_table.query(f'start_name == \"{g.name[0]}\" & end_name == \"{g.name[1]}\"').index.item())\n",
    "    df = df.groupby(['start_name', 'end_name']).apply(f)\n",
    "    df['section_id'] = df['section_id'].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def identify(df, dt_table, sec_table):\n",
    "    df = datetime2id(df, dt_table)\n",
    "    df = section2id(df, sec_table)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae9e67-47b0-4274-b963-edf85e4b3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = identify(df_test, dt_table, sec_table)\n",
    "df_test.to_pickle('./datasets_1h/kannetsu_210402-210531.pkl')\n",
    "\n",
    "df_all = identify(df_all, dt_table, sec_table)\n",
    "df_all.to_pickle('./datasets_1h/kannetsu_210402-220531.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a41bef-baf7-4240-a8cd-184ee4e8b9f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train, Testに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded5add-616d-47ab-b199-ade684840ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: 2021/4/2 - 2022/2/28\n",
    "# test: 2022/3/1 - 2022/5/31\n",
    "df_all = pd.read_pickle('./datasets_1h/kannetsu_210402-220531.pkl')\n",
    "\n",
    "sep_date = '2022/3/1'\n",
    "df_train = df_all.loc[df_all['datetime'] < pd.Timestamp(sep_date)]\n",
    "df_test = df_all.loc[df_all['datetime'] >= pd.Timestamp(sep_date)]\n",
    "\n",
    "df_train.to_pickle('./datasets_1h/kannetsu_210402-220228.pkl')\n",
    "df_test.to_pickle('./datasets_1h/kannetsu_220301-220531.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491ee0e-e075-4465-94ce-420715a72ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: 2021/4/2 - 2021/5/19\n",
    "# test: 2021/5/20 - 2021/5/31\n",
    "df_mini = pd.read_pickle('./datasets_1h/kannetsu_210402-210531.pkl')\n",
    "\n",
    "sep_date = '2021/5/20'\n",
    "df_train = df_mini.loc[df_mini['datetime'] < pd.Timestamp(sep_date)]\n",
    "df_test = df_mini.loc[df_mini['datetime'] >= pd.Timestamp(sep_date)]\n",
    "\n",
    "df_train.to_pickle('./datasets_1h/kannetsu_210402-210519.pkl')\n",
    "df_test.to_pickle('./datasets_1h/kannetsu_210520-210531.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a0e9b-14b1-434c-8de6-2aa603db4727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Spatial Temporal Matrixに整形\n",
    "- 区間数 x 時系列数 の行列\n",
    "- 実際は 区間数 x 時系列数 x 特徴量数 のテンソル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e22b3-fa73-4af8-83ae-dad170795ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_tr = pd.read_pickle('./datasets_1h/kannetsu_210402-210519.pkl')\n",
    "df_test_va = pd.read_pickle('./datasets_1h/kannetsu_210520-210531.pkl')\n",
    "\n",
    "df_all_tr = pd.read_pickle('./datasets_1h/kannetsu_210402-220228.pkl')\n",
    "df_all_va = pd.read_pickle('./datasets_1h/kannetsu_220301-220531.pkl')\n",
    "\n",
    "dt_table = pd.read_pickle(f'{config.TABLES_DIR}/datetime_table.pkl')\n",
    "sec_table = pd.read_pickle(f'{config.TABLES_DIR}/section_table.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39c58f-57dc-48c9-a557-0f5e71ead960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 特徴量の元になる列\n",
    "# time_col = ['month', 'hour', 'dayofweek', 'is_holiday']\n",
    "# section_col = ['direction', 'lane_count', 'KP']\n",
    "time_col = ['datetime_id']\n",
    "section_col = ['section_id']\n",
    "search_col = ['search_1h', 'search_unspec_1d']\n",
    "traffic_col = ['allCars']\n",
    "\n",
    "feature_col = time_col + section_col + search_col + traffic_col\n",
    "# feature_col = time_col + section_col + traffic_col\n",
    "# feature_col = search_col + traffic_col\n",
    "\n",
    "# 予測対象\n",
    "target_col = 'allCars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cfca28-8fa1-49f8-a821-5b7c5f49d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = format_stmatrix(df_all_tr, sec_table, feature_col, config.TARGET_COL)\n",
    "X_va, y_va = format_stmatrix(df_all_va, sec_table, feature_col, config.TARGET_COL)\n",
    "print(X_tr.shape, X_va.shape, y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffac485-c759-4a8c-be33-a714e2b4ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(X_tr, './datasets_1h/features_train.pkl')\n",
    "# torch.save(X_va, './datasets_1h/features_test.pkl')\n",
    "# torch.save(y_tr, './datasets_1h/labels_train.pkl')\n",
    "# torch.save(y_va, './datasets_1h/labels_test.pkl')\n",
    "\n",
    "# torch.save(X_tr, f'datasets_1h/mini_features_train.pkl')\n",
    "# torch.save(X_va, f'datasets_1h/mini_features_test.pkl')\n",
    "# torch.save(y_tr, f'datasets_1h/mini_labels_train.pkl')\n",
    "# torch.save(y_va, f'datasets_1h/mini_labels_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376cb0a-2709-4e34-94af-8c23af4a418f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 標準化・正規化\n",
    "- 標準化を行う\n",
    "- 時間特徴量（`month`, `hour`, `day_of_week`）はsin, cosで変換するのもやってみる\n",
    "- 検索数, 台数は上り・下り別でもやってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b43f0a-daa0-4011-8eb4-a3d1c155acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_table = pd.read_pickle(f'{config.TABLES_DIR}/datetime_table.pkl')\n",
    "sec_table = pd.read_pickle(f'{config.TABLES_DIR}/section_table.pkl')\n",
    "\n",
    "X_tr = torch.load('./datasets_1h/features_train.pkl')\n",
    "X_va = torch.load('./datasets_1h/features_test.pkl')\n",
    "y_tr = torch.load('./datasets_1h/labels_train.pkl')\n",
    "y_va = torch.load('./datasets_1h/labels_test.pkl')\n",
    "\n",
    "# X_tr = torch.load(f'datasets_1h/mini_features_train.pkl')\n",
    "# X_va = torch.load(f'datasets_1h/mini_features_test.pkl')\n",
    "# y_tr = torch.load(f'datasets_1h/mini_labels_train.pkl')\n",
    "# y_va = torch.load(f'datasets_1h/mini_labels_test.pkl')\n",
    "\n",
    "print(X_tr.shape, X_va.shape)\n",
    "print(y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f0469-e844-48d9-b897-4c4b2596f2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ID列は飛ばして標準化\n",
    "skip_features = [0, 1]\n",
    "scaler = STMatrixStandardScaler(skip_features=skip_features)\n",
    "\n",
    "scaler.fit(X_tr)\n",
    "X_tr_norm = scaler.transform(X_tr)\n",
    "\n",
    "scaler.fit(X_va)\n",
    "X_va_norm = scaler.transform(X_va)\n",
    "\n",
    "# torch.save(X_tr_norm, f'datasets_1h/features_train_norm.pkl')\n",
    "# torch.save(X_va_norm, f'datasets_1h/features_test_norm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b4d27-83ee-4e74-8cbd-e2f88b5bc7e3",
   "metadata": {},
   "source": [
    "## データセットの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "265d39fd-7d3b-47e2-96d6-aad31ba1ecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 1) (63, 4)\n",
      "torch.Size([5, 7992, 63]) torch.Size([5, 2208, 63])\n",
      "torch.Size([1, 7992, 63]) torch.Size([1, 2208, 63])\n"
     ]
    }
   ],
   "source": [
    "dt_table = pd.read_pickle(f'{config.TABLES_DIR}/datetime_table.pkl')\n",
    "sec_table = pd.read_pickle(f'{config.TABLES_DIR}/section_table.pkl')\n",
    "\n",
    "X_tr = torch.load(f'{config.DATASET_DIR}_1h/features_train_norm.pkl')\n",
    "X_va = torch.load(f'{config.DATASET_DIR}_1h/features_test_norm.pkl')\n",
    "y_tr = torch.load(f'{config.DATASET_DIR}_1h/labels_train.pkl')\n",
    "y_va = torch.load(f'{config.DATASET_DIR}_1h/labels_test.pkl')\n",
    "\n",
    "# X_tr = torch.load(f'datasets_1h/mini_features_train_norm.pkl')\n",
    "# X_va = torch.load(f'datasets_1h/mini_features_test_norm.pkl')\n",
    "# y_tr = torch.load(f'datasets_1h/mini_labels_train.pkl')\n",
    "# y_va = torch.load(f'datasets_1h/mini_labels_test.pkl')\n",
    "\n",
    "print(dt_table.shape, sec_table.shape)\n",
    "print(X_tr.shape, X_va.shape)\n",
    "print(y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "00a7120d-d3fc-41cf-a2cc-59d55670cda0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_step = 7 * 24\n",
    "space_window = None\n",
    "\n",
    "dataset_train = STDataset(X_tr, y_tr, \n",
    "                          time_step=time_step, \n",
    "                          space_window=space_window)\n",
    "\n",
    "dataset_valid = STDataset(X_va, y_va, \n",
    "                          time_step=time_step, \n",
    "                          space_window=space_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "29cf21c7-2744-4522-b8cb-34c8bb1585df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xy(dataset):\n",
    "    X, y = dataset[:]\n",
    "    # specify `cars` column\n",
    "    X = X[-1].numpy().astype(np.float64)\n",
    "    # next step only\n",
    "    y = y[:, 0].numpy().astype(np.float64)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def validate(predicted, target):\n",
    "    mae_mat = np.abs(predicted - target)\n",
    "    mae = mae_mat.mean()\n",
    "    \n",
    "    mape_mat = np.abs((predicted[target > 0] - target[target > 0]) / target[target > 0])\n",
    "    mape = mape_mat.mean()\n",
    "    \n",
    "    return mae, mape\n",
    "\n",
    "\n",
    "def multistep_validate(predicted, target, steps=[1,3,6,12,18,24]):\n",
    "    result = {}\n",
    "    \n",
    "    for step in steps:\n",
    "        t = step - 1\n",
    "        mae, mape = validate(predicted[:,t], target[:,t])\n",
    "        print(f'{step} hour ahead: MAE = {mae:.3f}, MAPE = {mape:.3f}')\n",
    "        \n",
    "        result[f'{step}_ahead'] = {'mae': mae, 'mape': mape}\n",
    "    \n",
    "    mae, mape = validate(predicted, target)\n",
    "    print(f'Whole: MAE = {mae:.3f}, MAPE = {mape:.3f}')\n",
    "        \n",
    "    result['whole'] = {'mae': mae, 'mape': mape}\n",
    "    return result\n",
    "\n",
    "\n",
    "def test_baseline(model, X_train, X_test, y_train, y_test, prediction_horizon=24):\n",
    "    fix_seed()\n",
    "    \n",
    "    X_train = copy.deepcopy(X_train)\n",
    "    X_test = copy.deepcopy(X_test)\n",
    "    \n",
    "    # train\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train[:, 0])\n",
    "    print(f\"Train Time: {time.time() - start:.3f} [sec]\")\n",
    "    \n",
    "    # test  \n",
    "    def test(X, y):\n",
    "        start = time.time()\n",
    "        predicted = []\n",
    "        \n",
    "        for i in range(prediction_horizon):\n",
    "            start_in_step = time.time()\n",
    "            y_pred = model.predict(X)\n",
    "            predicted.append(y_pred)\n",
    "            \n",
    "            X[:, :-1] = X[:, 1:]\n",
    "            X[:, -1] = y_pred\n",
    "            print(f\"{i+1} step ahead: {time.time() - start_in_step :.3f} [sec]\")\n",
    "\n",
    "        print(f'Prediction Time: {time.time() - start :.3f} [sec]')\n",
    "        \n",
    "        predicted = np.stack(predicted, axis=-1)\n",
    "        return predicted\n",
    "\n",
    "    predicted_train  = test(X_train, y_train)\n",
    "    mae, mape = validate(predicted_train, y_train)\n",
    "    print('-'*20, f'Train Loss: MAE = {mae :.3f}, MAPE = {mape :.3f}', '-'*20)\n",
    "    \n",
    "    predicted_test = test(X_test, y_test)\n",
    "    mae, mape = validate(predicted_test, y_test)\n",
    "    print('-'*20, f'Test Loss: MAE = {mae :.3f}, MAPE = {mape :.3f}', '-'*20)\n",
    "    \n",
    "    result = {\n",
    "        'model': model,\n",
    "        'pred_train': predicted_train,\n",
    "        'pred_test': predicted_test\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02883778-81d5-47e8-a3e5-c29822bfae00",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a932f4-1644-43fd-ab83-a3747db185ab",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1a20ebb-df8e-4b5b-a4fd-d3a52d21d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)\n",
    "\n",
    "svm_result = test_baseline(SVR(kernel='rbf'), train_X, test_X, train_y, test_y)\n",
    "\n",
    "mae, mape = validate(svm_result['pred_test'], test_y)\n",
    "print(f'MAE: {mae:.3f}, MAPE: {mape:.3f}')\n",
    "\n",
    "svm_result['test_mae'] = mae\n",
    "svm_result['test_mape'] = mape\n",
    "svm_result['parameters'] = {\n",
    "    'kernel': svm_result['model'].kernel,\n",
    "    'C': svm_result['model'].C,\n",
    "    'epsilon': svm_result['model'].epsilon,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "09e42cf3-b8d0-4689-82a3-e2e21e0579bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"svm_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(svm_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12dceb-9e9d-4cf9-8593-262017df202a",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9256190b-e208-4208-a25c-7d58604e70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "bd21cc69-17b6-40cd-800b-ea3afdde50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"svm_result.pkl\", \"rb\") as f:\n",
    "    svm_result = pickle.load(f)\n",
    "\n",
    "pred_train = svm_result['pred_train']\n",
    "pred_test = svm_result['pred_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b7707ee6-37b2-4f07-972d-822012959986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hour ahead: MAE = 69.259, MAPE = 0.289\n",
      "3 hour ahead: MAE = 176.134, MAPE = 1.651\n",
      "6 hour ahead: MAE = 489.040, MAPE = 1.355\n",
      "12 hour ahead: MAE = 1043.344, MAPE = 0.719\n",
      "18 hour ahead: MAE = 1178.286, MAPE = 0.748\n",
      "24 hour ahead: MAE = 237.167, MAPE = 0.926\n",
      "Whole: MAE = 730.764, MAPE = 0.908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1_ahead': {'mae': 69.25876155331018, 'mape': 0.2887398501691781},\n",
       " '3_ahead': {'mae': 176.13436495113012, 'mape': 1.6514071096626743},\n",
       " '6_ahead': {'mae': 489.03982073649223, 'mape': 1.3552707186600481},\n",
       " '12_ahead': {'mae': 1043.3439463744485, 'mape': 0.7186483611982102},\n",
       " '18_ahead': {'mae': 1178.2864264970542, 'mape': 0.7476537026136628},\n",
       " '24_ahead': {'mae': 237.16728531069282, 'mape': 0.9263745027309614},\n",
       " 'whole': {'mae': 730.764478673319, 'mape': 0.9077321727000494}}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multistep_validate(pred_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326b68d-3775-4278-9336-5f0a749ba1d5",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce7a61c9-14b0-42b9-93cd-b68db0e05f10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Time: 11114.791 [sec]\n",
      "1 step ahead: 0.630 [sec]\n",
      "2 step ahead: 0.146 [sec]\n",
      "3 step ahead: 0.139 [sec]\n",
      "4 step ahead: 0.137 [sec]\n",
      "5 step ahead: 0.139 [sec]\n",
      "6 step ahead: 0.138 [sec]\n",
      "7 step ahead: 0.139 [sec]\n",
      "8 step ahead: 0.138 [sec]\n",
      "9 step ahead: 0.139 [sec]\n",
      "10 step ahead: 0.138 [sec]\n",
      "11 step ahead: 0.139 [sec]\n",
      "12 step ahead: 0.137 [sec]\n",
      "13 step ahead: 0.136 [sec]\n",
      "14 step ahead: 0.135 [sec]\n",
      "15 step ahead: 0.135 [sec]\n",
      "16 step ahead: 0.135 [sec]\n",
      "17 step ahead: 0.137 [sec]\n",
      "18 step ahead: 0.136 [sec]\n",
      "19 step ahead: 0.137 [sec]\n",
      "20 step ahead: 0.136 [sec]\n",
      "21 step ahead: 0.137 [sec]\n",
      "22 step ahead: 0.139 [sec]\n",
      "23 step ahead: 0.142 [sec]\n",
      "24 step ahead: 0.146 [sec]\n",
      "Test Time: 3.813 [sec]\n",
      "Train Loss: MAE = 632.685, MAPE = 14083778.444\n",
      "1 step ahead: 0.181 [sec]\n",
      "2 step ahead: 0.041 [sec]\n",
      "3 step ahead: 0.038 [sec]\n",
      "4 step ahead: 0.038 [sec]\n",
      "5 step ahead: 0.038 [sec]\n",
      "6 step ahead: 0.039 [sec]\n",
      "7 step ahead: 0.039 [sec]\n",
      "8 step ahead: 0.039 [sec]\n",
      "9 step ahead: 0.039 [sec]\n",
      "10 step ahead: 0.039 [sec]\n",
      "11 step ahead: 0.038 [sec]\n",
      "12 step ahead: 0.038 [sec]\n",
      "13 step ahead: 0.038 [sec]\n",
      "14 step ahead: 0.038 [sec]\n",
      "15 step ahead: 0.037 [sec]\n",
      "16 step ahead: 0.038 [sec]\n",
      "17 step ahead: 0.038 [sec]\n",
      "18 step ahead: 0.038 [sec]\n",
      "19 step ahead: 0.039 [sec]\n",
      "20 step ahead: 0.038 [sec]\n",
      "21 step ahead: 0.038 [sec]\n",
      "22 step ahead: 0.038 [sec]\n",
      "23 step ahead: 0.039 [sec]\n",
      "24 step ahead: 0.041 [sec]\n",
      "Test Time: 1.070 [sec]\n",
      "Test Loss: MAE = 658.306, MAPE = 18354062.038\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)\n",
    "\n",
    "predicted_train, predicted_test = test_baseline(RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    criterion='mae',\n",
    "    # max_depth=10,\n",
    "    random_state=config.RANDOM_SEED,\n",
    "), train_X, test_X, train_y, test_y)\n",
    "\n",
    "rf_result = {\n",
    "    'predicted_train': predicted_train,\n",
    "    'predicted_test': predicted_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "847f0cd5-a137-4257-978b-3508b03dd2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Time: 10380.226 [sec]\n",
      "1 step ahead: 0.253 [sec]\n",
      "2 step ahead: 0.129 [sec]\n",
      "3 step ahead: 0.125 [sec]\n",
      "4 step ahead: 0.124 [sec]\n",
      "5 step ahead: 0.126 [sec]\n",
      "6 step ahead: 0.124 [sec]\n",
      "7 step ahead: 0.124 [sec]\n",
      "8 step ahead: 0.123 [sec]\n",
      "9 step ahead: 0.124 [sec]\n",
      "10 step ahead: 0.124 [sec]\n",
      "11 step ahead: 0.124 [sec]\n",
      "12 step ahead: 0.123 [sec]\n",
      "13 step ahead: 0.122 [sec]\n",
      "14 step ahead: 0.121 [sec]\n",
      "15 step ahead: 0.122 [sec]\n",
      "16 step ahead: 0.122 [sec]\n",
      "17 step ahead: 0.124 [sec]\n",
      "18 step ahead: 0.124 [sec]\n",
      "19 step ahead: 0.124 [sec]\n",
      "20 step ahead: 0.123 [sec]\n",
      "21 step ahead: 0.124 [sec]\n",
      "22 step ahead: 0.125 [sec]\n",
      "23 step ahead: 0.129 [sec]\n",
      "24 step ahead: 0.132 [sec]\n",
      "Prediction Time: 3.114 [sec]\n",
      "-------------------- Train Loss: MAE = 633.889, MAPE = 3.062 --------------------\n",
      "1 step ahead: 0.076 [sec]\n",
      "2 step ahead: 0.038 [sec]\n",
      "3 step ahead: 0.036 [sec]\n",
      "4 step ahead: 0.036 [sec]\n",
      "5 step ahead: 0.036 [sec]\n",
      "6 step ahead: 0.036 [sec]\n",
      "7 step ahead: 0.036 [sec]\n",
      "8 step ahead: 0.036 [sec]\n",
      "9 step ahead: 0.035 [sec]\n",
      "10 step ahead: 0.036 [sec]\n",
      "11 step ahead: 0.035 [sec]\n",
      "12 step ahead: 0.035 [sec]\n",
      "13 step ahead: 0.036 [sec]\n",
      "14 step ahead: 0.035 [sec]\n",
      "15 step ahead: 0.035 [sec]\n",
      "16 step ahead: 0.035 [sec]\n",
      "17 step ahead: 0.036 [sec]\n",
      "18 step ahead: 0.036 [sec]\n",
      "19 step ahead: 0.036 [sec]\n",
      "20 step ahead: 0.036 [sec]\n",
      "21 step ahead: 0.036 [sec]\n",
      "22 step ahead: 0.036 [sec]\n",
      "23 step ahead: 0.037 [sec]\n",
      "24 step ahead: 0.038 [sec]\n",
      "Prediction Time: 0.905 [sec]\n",
      "-------------------- Test Loss: MAE = 659.304, MAPE = 3.392 --------------------\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    criterion='mae',\n",
    "    max_depth=10,\n",
    "    random_state=config.RANDOM_SEED,\n",
    ")\n",
    "\n",
    "rf_result = test_baseline(model, train_X, test_X, train_y, test_y)\n",
    "\n",
    "mae, mape = validate(rf_result['pred_test'], test_y)\n",
    "\n",
    "rf_result['test_mae'] = mae\n",
    "rf_result['test_mape'] = mape\n",
    "rf_result['parameters'] = {\n",
    "    'n_estimators': rf_result['model'].n_estimators,\n",
    "    'criterion': rf_result['model'].criterion,\n",
    "    'max_depth': rf_result['model'].max_depth,\n",
    "    'random_state': rf_result['model'].random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f4d59271-0c94-4b27-a48e-c40b60e1a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rf_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008808f8-5b23-4fce-83ef-e0601e05b523",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2fb028e2-7d86-40fb-9f5c-d32f97204524",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "4f10faba-22fd-4e06-9444-f0818b744c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rf_result.pkl\", \"rb\") as f:\n",
    "    rf_result = pickle.load(f)\n",
    "\n",
    "pred_train = rf_result['pred_train']\n",
    "pred_test = rf_result['pred_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "571169c1-7070-4ed4-93c2-4c4ce588ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hour ahead: MAE = 39.427, MAPE = 0.136\n",
      "3 hour ahead: MAE = 549.770, MAPE = 4.299\n",
      "6 hour ahead: MAE = 541.830, MAPE = 2.915\n",
      "12 hour ahead: MAE = 796.942, MAPE = 0.912\n",
      "18 hour ahead: MAE = 915.586, MAPE = 0.915\n",
      "24 hour ahead: MAE = 400.149, MAPE = 2.076\n",
      "Whole: MAE = 659.304, MAPE = 1.670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1_ahead': {'mae': 39.42694633408919, 'mape': 0.13588226911912749},\n",
       " '3_ahead': {'mae': 549.7704686318973, 'mape': 4.298510286520116},\n",
       " '6_ahead': {'mae': 541.8295304232804, 'mape': 2.915203649805911},\n",
       " '12_ahead': {'mae': 796.9419170445956, 'mape': 0.9122415001349675},\n",
       " '18_ahead': {'mae': 915.5858616780046, 'mape': 0.9146244189289804},\n",
       " '24_ahead': {'mae': 400.14889172335603, 'mape': 2.076270267026097},\n",
       " 'whole': {'mae': 659.3037211829177, 'mape': 1.6695408038717126}}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multistep_validate(pred_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15d17d-e171-4d2b-91c1-b8cf940127b9",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "da84c018-0040-436c-bc50-110511acc1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7992, 63]) torch.Size([5, 2208, 63])\n",
      "torch.Size([1, 7992, 63]) torch.Size([1, 2208, 63])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.load(f'{config.DATASET_DIR}_1h/features_train.pkl')\n",
    "X_va = torch.load(f'{config.DATASET_DIR}_1h/features_test.pkl')\n",
    "y_tr = torch.load(f'{config.DATASET_DIR}_1h/labels_train.pkl')\n",
    "y_va = torch.load(f'{config.DATASET_DIR}_1h/labels_test.pkl')\n",
    "\n",
    "print(X_tr.shape, X_va.shape)\n",
    "print(y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "4bd264a4-ee64-4065-b2d4-66d35f4901cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_step = 7 * 24\n",
    "space_window = None\n",
    "\n",
    "dataset_train = STDataset(X_tr, y_tr, \n",
    "                          time_step=time_step, \n",
    "                          space_window=space_window)\n",
    "\n",
    "dataset_valid = STDataset(X_va, y_va, \n",
    "                          time_step=time_step, \n",
    "                          space_window=space_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "30d3dd01-565e-4a96-a5bc-cce1b3079ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_arima(X_train, X_test, y_train, y_test, prediction_horizon=24):\n",
    "    fix_seed()\n",
    "    \n",
    "    X_train = copy.deepcopy(X_train)\n",
    "    X_test = copy.deepcopy(X_test)\n",
    "    \n",
    "    def test(X, y):\n",
    "        start = time.time()\n",
    "        predicted = []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            model = ARIMA(X[i], order=(1,1,0))\n",
    "            model_fitted = model.fit()\n",
    "            \n",
    "            y_pred = model_fitted.forecast(steps=prediction_horizon)\n",
    "            predicted.append(y_pred)\n",
    "            \n",
    "            if (i+1) % 500 == 0:\n",
    "                mae, mape = validate(y_pred, y[i])\n",
    "                print(f'Step {i+1}: MAE = {mae:.3f}, MAPE = {mape:.3f} ({time.time() - start :.3f} [sec])')\n",
    "        \n",
    "        print(f'Prediction Time: {time.time() - start :.3f} [sec]')\n",
    "        \n",
    "        predicted = np.stack(predicted, axis=0)\n",
    "        return predicted\n",
    "    \n",
    "    \n",
    "    predicted_train = test(X_train, y_train)\n",
    "    mae, mape = validate(predicted_train, y_train)\n",
    "    print('-'*20, f'Train Loss: MAE = {mae :.3f}, MAPE = {mape :.3f}', '-'*20)\n",
    "    \n",
    "    predicted_test = test(X_test, y_test)\n",
    "    mae, mape = validate(predicted_test, y_test)\n",
    "    print('-'*20, f'Test Loss: MAE = {mae :.3f}, MAPE = {mape :.3f}', '-'*20)\n",
    "    \n",
    "    result = {\n",
    "        'pred_train': predicted_train,\n",
    "        'pred_test': predicted_test\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "df03fb4b-e717-4c97-b7a3-48bf1c01e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: MAE = 231.020, MAPE = 0.624 (14.684 [sec])\n",
      "Step 1000: MAE = 236.792, MAPE = 0.664 (29.207 [sec])\n",
      "Step 1500: MAE = 224.494, MAPE = 0.640 (43.836 [sec])\n",
      "Step 2000: MAE = 130.527, MAPE = 0.542 (57.731 [sec])\n",
      "Step 2500: MAE = 158.923, MAPE = 0.579 (71.409 [sec])\n",
      "Step 3000: MAE = 117.023, MAPE = 0.561 (84.664 [sec])\n",
      "Step 3500: MAE = 247.868, MAPE = 0.617 (99.343 [sec])\n",
      "Step 4000: MAE = 415.245, MAPE = 0.617 (111.840 [sec])\n",
      "Step 4500: MAE = 565.551, MAPE = 0.630 (123.217 [sec])\n",
      "Step 5000: MAE = 792.704, MAPE = 0.681 (133.288 [sec])\n",
      "Step 5500: MAE = 706.618, MAPE = 0.646 (142.023 [sec])\n",
      "Step 6000: MAE = 586.720, MAPE = 0.657 (150.553 [sec])\n",
      "Step 6500: MAE = 1706.745, MAPE = 1.061 (159.724 [sec])\n",
      "Step 7000: MAE = 622.574, MAPE = 0.525 (169.495 [sec])\n",
      "Step 7500: MAE = 1942.341, MAPE = 1.039 (179.121 [sec])\n",
      "Step 8000: MAE = 1217.825, MAPE = 0.628 (188.085 [sec])\n",
      "Step 8500: MAE = 1367.543, MAPE = 0.595 (196.781 [sec])\n",
      "Step 9000: MAE = 1571.745, MAPE = 0.593 (205.663 [sec])\n",
      "Step 9500: MAE = 1422.890, MAPE = 0.614 (214.412 [sec])\n",
      "Step 10000: MAE = 1426.227, MAPE = 0.654 (222.987 [sec])\n",
      "Step 10500: MAE = 1644.461, MAPE = 0.602 (231.643 [sec])\n",
      "Step 11000: MAE = 1382.307, MAPE = 0.604 (240.141 [sec])\n",
      "Step 11500: MAE = 1722.512, MAPE = 0.684 (248.639 [sec])\n",
      "Step 12000: MAE = 1315.471, MAPE = 0.617 (257.243 [sec])\n",
      "Step 12500: MAE = 934.036, MAPE = 0.660 (265.688 [sec])\n",
      "Step 13000: MAE = 987.726, MAPE = 0.601 (274.222 [sec])\n",
      "Step 13500: MAE = 700.324, MAPE = 0.531 (283.032 [sec])\n",
      "Step 14000: MAE = 1271.816, MAPE = 0.675 (291.606 [sec])\n",
      "Step 14500: MAE = 931.284, MAPE = 0.651 (300.286 [sec])\n",
      "Step 15000: MAE = 864.642, MAPE = 0.656 (309.300 [sec])\n",
      "Step 15500: MAE = 854.443, MAPE = 0.660 (318.528 [sec])\n",
      "Step 16000: MAE = 301.489, MAPE = 0.525 (329.989 [sec])\n",
      "Step 16500: MAE = 213.458, MAPE = 0.434 (343.091 [sec])\n",
      "Step 17000: MAE = 312.882, MAPE = 0.473 (356.722 [sec])\n",
      "Step 17500: MAE = 316.169, MAPE = 0.548 (370.071 [sec])\n",
      "Step 18000: MAE = 133.268, MAPE = 0.264 (383.187 [sec])\n",
      "Step 18500: MAE = 29.323, MAPE = 0.178 (396.027 [sec])\n",
      "Step 19000: MAE = 65.036, MAPE = 0.370 (408.372 [sec])\n",
      "Step 19500: MAE = 92.145, MAPE = 0.700 (421.566 [sec])\n",
      "Step 20000: MAE = 107.265, MAPE = 0.492 (435.640 [sec])\n",
      "Prediction Time: 448.764 [sec]\n",
      "-------------------- Train Loss: MAE = 692.985, MAPE = 0.620 --------------------\n",
      "Step 500: MAE = 194.750, MAPE = 0.401 (14.610 [sec])\n",
      "Step 1000: MAE = 539.883, MAPE = 0.749 (28.316 [sec])\n",
      "Step 1500: MAE = 836.507, MAPE = 0.631 (37.936 [sec])\n",
      "Step 2000: MAE = 916.345, MAPE = 0.565 (47.250 [sec])\n",
      "Step 2500: MAE = 1672.256, MAPE = 0.750 (56.169 [sec])\n",
      "Step 3000: MAE = 1300.953, MAPE = 0.649 (65.258 [sec])\n",
      "Step 3500: MAE = 1808.185, MAPE = 0.715 (73.825 [sec])\n",
      "Step 4000: MAE = 865.776, MAPE = 0.641 (82.635 [sec])\n",
      "Step 4500: MAE = 170.446, MAPE = 0.491 (95.836 [sec])\n",
      "Step 5000: MAE = 88.637, MAPE = 0.334 (109.215 [sec])\n",
      "Prediction Time: 117.132 [sec]\n",
      "-------------------- Test Loss: MAE = 734.508, MAPE = 0.611 --------------------\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)\n",
    "\n",
    "arima_result = test_arima(train_X, test_X, train_y, test_y)\n",
    "\n",
    "mae, mape = validate(arima_result['pred_test'], test_y)\n",
    "\n",
    "arima_result['test_mae'] = mae\n",
    "arima_result['test_mape'] = mape\n",
    "arima_result['parameters'] = {\n",
    "    'order': (1,1,0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ed39cbc3-17fb-470e-9e61-fde06f637557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 734.508, MAPE: 0.611\n"
     ]
    }
   ],
   "source": [
    "mae, mape = validate(arima_result['pred_test'], test_y)\n",
    "print(f'MAE: {mae:.3f}, MAPE: {mape:.3f}')\n",
    "\n",
    "# arima_result['test_mae'] = mae\n",
    "# arima_result['test_mape'] = mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7242350c-3b8b-4caa-a29f-9edb5995b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arima_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(arima_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd7dd5-5d6a-4e0c-8d10-919329c5d6b3",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "80ab13bc-b7e3-4c9d-be59-5a389b4444a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8c07eda5-8acb-406d-9d14-638ef6ef2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"arima_result.pkl\", \"rb\") as f:\n",
    "    arima_result = pickle.load(f)\n",
    "\n",
    "pred_train = arima_result['pred_train']\n",
    "pred_test = arima_result['pred_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "381ff324-2881-4b2e-a4fd-a91e7a22b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hour ahead: MAE = 72.531, MAPE = 0.273\n",
      "3 hour ahead: MAE = 130.115, MAPE = 0.619\n",
      "6 hour ahead: MAE = 397.821, MAPE = 0.500\n",
      "12 hour ahead: MAE = 1096.759, MAPE = 0.724\n",
      "18 hour ahead: MAE = 1238.566, MAPE = 0.741\n",
      "24 hour ahead: MAE = 179.432, MAPE = 0.375\n",
      "Whole: MAE = 734.508, MAPE = 0.611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1_ahead': {'mae': 72.53137002657692, 'mape': 0.2733957343726342},\n",
       " '3_ahead': {'mae': 130.1147550862505, 'mape': 0.6187721420828514},\n",
       " '6_ahead': {'mae': 397.8210558658763, 'mape': 0.49980179844191724},\n",
       " '12_ahead': {'mae': 1096.7594203810447, 'mape': 0.724345490915564},\n",
       " '18_ahead': {'mae': 1238.5662754946195, 'mape': 0.7413433892137449},\n",
       " '24_ahead': {'mae': 179.43235944351275, 'mape': 0.3746089440616245},\n",
       " 'whole': {'mae': 734.5076539773044, 'mape': 0.6106473270619756}}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multistep_validate(pred_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bad09c-5606-47a5-8698-a554176d6bce",
   "metadata": {},
   "source": [
    "## HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d21e449a-9cc7-4395-bd1a-2ed61e2bb079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7992, 63]) torch.Size([5, 2208, 63])\n",
      "torch.Size([1, 7992, 63]) torch.Size([1, 2208, 63])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.load(f'{config.DATASET_DIR}_1h/features_train.pkl')\n",
    "X_va = torch.load(f'{config.DATASET_DIR}_1h/features_test.pkl')\n",
    "y_tr = torch.load(f'{config.DATASET_DIR}_1h/labels_train.pkl')\n",
    "y_va = torch.load(f'{config.DATASET_DIR}_1h/labels_test.pkl')\n",
    "\n",
    "print(X_tr.shape, X_va.shape)\n",
    "print(y_tr.shape, y_va.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d7cdae55-be37-47f9-a083-c780679b03e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_step = 7 * 24\n",
    "space_window = None\n",
    "\n",
    "dataset_train = STDataset(X_tr, y_tr, \n",
    "                          time_step=time_step, \n",
    "                          space_window=space_window)\n",
    "\n",
    "dataset_valid = STDataset(X_va, y_va, \n",
    "                          time_step=time_step, \n",
    "                          space_window=space_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5365b3-961d-48d7-a500-0063d9cca9f7",
   "metadata": {},
   "source": [
    "### simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f6b1cd64-8020-4a10-bbd7-09b5924a6a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20475,)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e9552bd2-5007-47eb-a5ea-bec3c53d22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)\n",
    "\n",
    "T, P = train_X.shape[1], train_y.shape[1]\n",
    "\n",
    "train_y_pred = np.zeros_like(train_y)\n",
    "test_y_pred = np.zeros_like(test_y)\n",
    "\n",
    "for i in range(P):\n",
    "    train_y_pred[:, i] = train_X.mean(axis=1)\n",
    "    test_y_pred[:, i] = test_X.mean(axis=1)\n",
    "    \n",
    "    train_X[:, :-1] = train_X[:, 1:]\n",
    "    train_X[:, -1] = train_y_pred[:, i]\n",
    "    \n",
    "    test_X[:, :-1] = test_X[:, 1:]\n",
    "    test_X[:, -1] = test_y_pred[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8e29db7a-dc85-445e-bc15-4302f5c577b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497.45494560218503 0.9930123774842203\n"
     ]
    }
   ],
   "source": [
    "mae, mape = validate(test_y_pred, test_y)\n",
    "print(mae, mape)\n",
    "\n",
    "simple_ha_result = {\n",
    "    'pred_train': train_y_pred,\n",
    "    'pred_test': test_y_pred,\n",
    "    'test_mae': mae,\n",
    "    'test_mape': mape\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a3343005-87c3-4e77-af1c-999a7d427b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"simple_ha_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(simple_ha_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "f7de4067-3412-4bb9-a949-8a511f6e35e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497.45494560218503 0.9930123774842203\n",
      "1 hour ahead: MAE = 669.332, MAPE = 2.254\n",
      "3 hour ahead: MAE = 779.770, MAPE = 3.504\n",
      "6 hour ahead: MAE = 443.000, MAPE = 1.293\n",
      "12 hour ahead: MAE = 396.632, MAPE = 0.299\n",
      "18 hour ahead: MAE = 561.789, MAPE = 0.361\n",
      "24 hour ahead: MAE = 573.275, MAPE = 1.402\n",
      "Whole: MAE = 497.455, MAPE = 0.993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1_ahead': {'mae': 669.3324436255984, 'mape': 2.254447207525798},\n",
       " '3_ahead': {'mae': 779.7700424356652, 'mape': 3.503969348643898},\n",
       " '6_ahead': {'mae': 442.999644351576, 'mape': 1.293133465690125},\n",
       " '12_ahead': {'mae': 396.6318815468111, 'mape': 0.2991005106631112},\n",
       " '18_ahead': {'mae': 561.7887730093458, 'mape': 0.36098352543586737},\n",
       " '24_ahead': {'mae': 573.2754977299062, 'mape': 1.4024852432698425},\n",
       " 'whole': {'mae': 497.45494560218503, 'mape': 0.9930123774842203}}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae, mape = validate(test_y_pred, test_y)\n",
    "print(mae, mape)\n",
    "\n",
    "multistep_validate(test_y_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3da5d-6ae9-4431-a45d-7013bc03ef4f",
   "metadata": {},
   "source": [
    "### periodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "84503f13-f76b-4fb2-b17a-ed8b34e66722",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)\n",
    "\n",
    "T, P = train_X.shape[1], train_y.shape[1]\n",
    "\n",
    "train_y_pred = np.zeros_like(train_y)\n",
    "test_y_pred = np.zeros_like(test_y)\n",
    "\n",
    "for i in range(P):\n",
    "    periodic_ind = np.arange(i, T, P)\n",
    "    train_y_pred[:, i] = train_X[:, periodic_ind].mean(axis=1)\n",
    "    test_y_pred[:, i] = test_X[:, periodic_ind].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f606e7aa-e143-489f-be18-ab18a7d0bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159.87270093222477 0.22896403001170956\n"
     ]
    }
   ],
   "source": [
    "mae, mape = validate(test_y_pred, test_y)\n",
    "print(mae, mape)\n",
    "\n",
    "ha_result = {\n",
    "    'pred_train': train_y_pred,\n",
    "    'pred_test': test_y_pred,\n",
    "    'test_mae': mae,\n",
    "    'test_mape': mape\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0e1c199c-74e4-4a7e-8f52-cea95e267e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ha_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ha_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43650c0c-41df-4180-a08a-48592e96e805",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "07be6e73-b7ea-4f24-97df-36ca231eeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_xy(dataset_train)\n",
    "test_X, test_y = get_xy(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f5fe4f38-331e-4c32-b6bf-b817dc5bc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ha_result.pkl\", \"rb\") as f:\n",
    "    ha_result = pickle.load(f)\n",
    "\n",
    "pred_train = ha_result['pred_train']\n",
    "pred_test = ha_result['pred_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "da940310-3fe2-4427-b7dd-7a1167bcf3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 hour ahead: MAE = 61.928, MAPE = 0.246\n",
      "3 hour ahead: MAE = 39.024, MAPE = 0.187\n",
      "6 hour ahead: MAE = 178.838, MAPE = 0.322\n",
      "12 hour ahead: MAE = 178.033, MAPE = 0.189\n",
      "18 hour ahead: MAE = 202.534, MAPE = 0.202\n",
      "24 hour ahead: MAE = 88.175, MAPE = 0.233\n",
      "Whole: MAE = 159.873, MAPE = 0.229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1_ahead': {'mae': 61.92832847424684, 'mape': 0.24617891681915038},\n",
       " '3_ahead': {'mae': 39.02397149335925, 'mape': 0.18679314031056568},\n",
       " '6_ahead': {'mae': 178.83848936399957, 'mape': 0.32200395961439604},\n",
       " '12_ahead': {'mae': 178.03344671201813, 'mape': 0.18893681733066764},\n",
       " '18_ahead': {'mae': 202.53422956484178, 'mape': 0.20236932574865693},\n",
       " '24_ahead': {'mae': 88.17535903250187, 'mape': 0.23274982578151346},\n",
       " 'whole': {'mae': 159.87270093222477, 'mape': 0.22896403001170956}}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multistep_validate(pred_test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "58fb1a7f-1930-4809-a140-d6f1e29be293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(1, TRAFFIC_HIDDEN, TRAFFIC_LSTM_LAYERS, bidirectional=bidirectional, dropout=0.4, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, T, S = x.shape\n",
    "        x = x[..., [S // 2]]\n",
    "        # N x T x 1 -> N x T x H, (L x N x H, L x N x H)\n",
    "        outs, (h, c) = self.lstm(x)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # 2*L_t x N x H_t -> L_t x N x 2*H_t\n",
    "            L2, N, H_t = h.shape\n",
    "            h = h.transpose(0,1).reshape(N, L2 // 2, -1).transpose(0,1).contiguous()\n",
    "            c = h.transpose(0,1).reshape(N, L2 // 2, -1).transpose(0,1).contiguous()    \n",
    "        \n",
    "        return outs, (h, c)\n",
    "    \n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_ratio = 0.3\n",
    "        \n",
    "        self.hid_dim = 2 * TRAFFIC_HIDDEN if bidirectional else TRAFFIC_HIDDEN\n",
    "        \n",
    "        self.lstm = nn.LSTM(1, self.hid_dim, TRAFFIC_LSTM_LAYERS, dropout=self.dropout_ratio, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hid_dim, FC_EMB)\n",
    "        self.fc2 = nn.Linear(FC_EMB, 1)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        N, _, P = x.shape\n",
    "        \n",
    "        # N x C=1 x P -> N x P x C=1\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # N x P x C, (L x N x H_t, L x N x H_t) -> N x P x H_t, (L x N x H_t, L x N x H_t)\n",
    "        outs, state = self.lstm(x, state)\n",
    "        outs = F.relu(self.fc1(outs))\n",
    "        outs = self.fc2(outs)\n",
    "\n",
    "        return outs, state\n",
    "    \n",
    "    def generate(self, trf_enc, start_value=-1.0):\n",
    "        with torch.no_grad():\n",
    "            # N x 1 x 1\n",
    "            N = trf_enc[0].shape[1]\n",
    "            out = torch.tensor(start_value).repeat(N).unsqueeze(-1).unsqueeze(-1)\n",
    "            out = out.to(trf_enc[0].device)\n",
    "            \n",
    "            state = trf_enc\n",
    "            \n",
    "            generated = []\n",
    "\n",
    "            for i in range(24):\n",
    "                out, state = self.forward(out, state)\n",
    "                generated.append(out)\n",
    "        \n",
    "        # N x P x 1\n",
    "        generated = torch.cat(generated, dim=1)\n",
    "        # N x P x 1 -> N x P\n",
    "        generated = generated[..., 0]\n",
    "        return generated\n",
    "    \n",
    "\n",
    "class LSTMOnlyEncoderDecoder(nn.Module):\n",
    "    def __init__(self, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = LSTMEncoder(bidirectional=bidirectional)\n",
    "        self.decoder = LSTMDecoder(bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, features, decoder_xs):\n",
    "        dt, rd, sr, un_sr, trf = features\n",
    "        \n",
    "        outs_trf, state_trf = self.encoder(trf)\n",
    "        outs, _ = self.decoder(decoder_xs, state_trf)\n",
    "\n",
    "        return outs\n",
    "    \n",
    "    def generate(self, features, start_value=-1.0):\n",
    "        dt, rd, sr, un_sr, trf = features\n",
    "        \n",
    "        outs_trf, state_trf = self.encoder(trf)    \n",
    "        generated = self.decoder.generate(state_trf, start_value)\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c133fd4f-c770-43ff-a474-9310a6ca376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 168, 5]) torch.Size([16, 1, 24])\n"
     ]
    }
   ],
   "source": [
    "(dt, rd, sr, un_sr, trf), labels = dataset_train[:16]\n",
    "print(trf.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e5677be3-bb12-4269-83c4-45d20e1bdd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 168, 256]) torch.Size([2, 16, 256])\n",
      "torch.Size([16, 24, 1])\n",
      "torch.Size([16, 24])\n"
     ]
    }
   ],
   "source": [
    "(outs_trf, state_trf) = LSTMEncoder()(trf)\n",
    "print(outs_trf.shape, state_trf[0].shape)\n",
    "\n",
    "decoded, _ = LSTMDecoder()(labels, state_trf)\n",
    "print(decoded.shape)\n",
    "\n",
    "predicted = LSTMOnlyEncoderDecoder().generate((dt, rd, sr, un_sr, trf))\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "eda27573-7e41-4fe1-b124-776a8b9c510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = dataset_train[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "2fbf221b-3152-444c-b961-282619b5f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_xs = torch.full_like(labels, -1)\n",
    "decoder_xs[..., 1:] = labels[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f8673a56-0cd2-45e8-87c5-b548131fc50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 24, 1])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = LSTMOnlyEncoderDecoder()((dt, rd, sr, un_sr, trf), decoder_xs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "b1c8960e-726f-46ba-8066-58af61dba69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMEncoder(nn.Module):\n",
    "    def __init__(self, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, TRAFFIC_CONV, TRAFFIC_KERNEL, padding=(TRAFFIC_KERNEL[0]//2, 0), padding_mode='replicate')\n",
    "        self.lstm = nn.LSTM(TRAFFIC_CONV, TRAFFIC_HIDDEN, TRAFFIC_LSTM_LAYERS, bidirectional=bidirectional, dropout=0.4, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, T, S = x.shape\n",
    "        \n",
    "        out = F.relu(self.conv(x.unsqueeze(1)))\n",
    "        # N x C x T -> N x T x C\n",
    "        out = out[..., 0].permute(0, 2, 1)\n",
    "        # N x T x C -> N x T x H, (L x N x H, L x N x H)\n",
    "        outs, (h, c) = self.lstm(out)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # 2*L_t x N x H_t -> L_t x N x 2*H_t\n",
    "            L2, N, H_t = h.shape\n",
    "            h = h.transpose(0,1).reshape(N, L2 // 2, -1).transpose(0,1).contiguous()\n",
    "            c = h.transpose(0,1).reshape(N, L2 // 2, -1).transpose(0,1).contiguous()\n",
    "        \n",
    "        return outs, (h, c)\n",
    "    \n",
    "\n",
    "class CNNLSTMEncoderDecoder(LSTMOnlyEncoderDecoder):\n",
    "    def __init__(self, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = CNNLSTMEncoder(bidirectional=bidirectional)\n",
    "        self.decoder = LSTMDecoder(bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0e9b1d52-d9e7-4f57-a81a-cd247146772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 168, 5]) torch.Size([16, 1, 24])\n"
     ]
    }
   ],
   "source": [
    "(dt, rd, sr, un_sr, trf), labels = dataset_train[:16]\n",
    "print(trf.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "d6509e36-c3ff-4b37-94e3-d0de96214f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 168, 256]) torch.Size([2, 16, 256])\n",
      "torch.Size([16, 24, 1])\n",
      "torch.Size([16, 24])\n"
     ]
    }
   ],
   "source": [
    "(outs_trf, state_trf) = CNNLSTMEncoder()(trf)\n",
    "print(outs_trf.shape, state_trf[0].shape)\n",
    "\n",
    "decoded, _ = LSTMDecoder()(labels, state_trf)\n",
    "print(decoded.shape)\n",
    "\n",
    "predicted = LSTMOnlyEncoderDecoder().generate((dt, rd, sr, un_sr, trf))\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "918a5424-9679-4872-9711-0f6b89075c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = dataset_train[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "8ddddfbb-91c3-4031-83e5-e1b39c02371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_xs = torch.full_like(labels, -1)\n",
    "decoder_xs[..., 1:] = labels[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "67fb2a06-de2a-427b-aeb9-9108a3c75451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 24, 1])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = CNNLSTMEncoderDecoder()((dt, rd, sr, un_sr, trf), decoder_xs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "24700f3b-c7c1-44c7-81cc-9e1d8d51196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEmbeddingDecoder(nn.Module):\n",
    "    def __init__(self, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_ratio = 0.3\n",
    "        \n",
    "        self.hid_dim = 2 * TRAFFIC_HIDDEN if bidirectional else TRAFFIC_HIDDEN\n",
    "        \n",
    "        self.datetime_embedding = CategoricalEmbedding(config.DT_TABLE_SIZE, DATETIME_EMB)\n",
    "        self.road_embedding = CategoricalEmbedding(config.SEC_TABLE_SIZE, ROAD_EMB)\n",
    "        self.emb_dropout = nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.lstm = nn.LSTM(1, self.hid_dim, TRAFFIC_LSTM_LAYERS, dropout=self.dropout_ratio, batch_first=True)\n",
    "        self.fc1 = nn.Linear(self.hid_dim + DATETIME_EMB + ROAD_EMB, FC_EMB)\n",
    "        self.fc2 = nn.Linear(FC_EMB, 1)\n",
    "\n",
    "    def forward(self, x, state, dt, rd):\n",
    "        N, _, P = x.shape\n",
    "        \n",
    "        # N x C=1 x P -> N x P x C=1\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # N x P x C, (L x N x H_t, L x N x H_t) -> N x P x H_t, (L x N x H_t, L x N x H_t)\n",
    "        outs, state = self.lstm(x, state)\n",
    "        \n",
    "        # N x P -> N x P x H_d\n",
    "        dt_emb = self.datetime_embedding(dt)\n",
    "        dt_emb = self.emb_dropout(dt_emb)\n",
    "        # N x 1 -> N x 1 x H_r\n",
    "        rd_emb = self.road_embedding(rd)\n",
    "        rd_emb = self.emb_dropout(rd_emb)\n",
    "        \n",
    "        outs = torch.cat([\n",
    "            dt_emb, \n",
    "            rd_emb.repeat(1, P, 1), \n",
    "            outs\n",
    "        ], dim=-1)\n",
    "        \n",
    "        outs = F.relu(self.fc1(outs))\n",
    "        outs = self.fc2(outs)\n",
    "\n",
    "        return outs, state\n",
    "    \n",
    "    def generate(self, trf_enc, dt, rd, start_value=-1.0):\n",
    "        with torch.no_grad():\n",
    "            # N x 1 x 1\n",
    "            N = trf_enc[0].shape[1]\n",
    "            out = torch.tensor(start_value).repeat(N).unsqueeze(-1).unsqueeze(-1)\n",
    "            out = out.to(trf_enc[0].device)\n",
    "            \n",
    "            state = trf_enc\n",
    "            \n",
    "            generated = []\n",
    "\n",
    "            for i in range(24):\n",
    "                out, state = self.forward(out, state, dt[:, [i]], rd)\n",
    "                generated.append(out)\n",
    "        \n",
    "        # N x P x 1\n",
    "        generated = torch.cat(generated, dim=1)\n",
    "        # N x P x 1 -> N x P\n",
    "        generated = generated[..., 0]\n",
    "        return generated\n",
    "    \n",
    "\n",
    "class CNNLSTMEmbeddingEncoderDecoder(nn.Module):\n",
    "    def __init__(self, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = CNNLSTMEncoder(bidirectional=bidirectional)\n",
    "        self.decoder = LSTMEmbeddingDecoder(bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, features, decoder_xs):\n",
    "        dt, rd, sr, un_sr, trf = features\n",
    "        \n",
    "        outs_trf, state_trf = self.encoder(trf)\n",
    "        outs, _ = self.decoder(decoder_xs, state_trf, dt, rd)\n",
    "\n",
    "        return outs\n",
    "    \n",
    "    def generate(self, features, start_value=-1.0):\n",
    "        dt, rd, sr, un_sr, trf = features\n",
    "        \n",
    "        outs_trf, state_trf = self.encoder(trf)    \n",
    "        generated = self.decoder.generate(state_trf, dt, rd, start_value)\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "e3f07a64-9ac0-4a1a-92a0-3fc9891443a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 168, 5]) torch.Size([16, 1, 24])\n"
     ]
    }
   ],
   "source": [
    "(dt, rd, sr, un_sr, trf), labels = dataset_train[:16]\n",
    "print(trf.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "2af0fe10-18af-4773-8716-1be2699afa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 168, 256]) torch.Size([2, 16, 256])\n",
      "torch.Size([16, 24, 1])\n",
      "torch.Size([16, 24])\n"
     ]
    }
   ],
   "source": [
    "(outs_trf, state_trf) = CNNLSTMEncoder()(trf)\n",
    "print(outs_trf.shape, state_trf[0].shape)\n",
    "\n",
    "decoded, _ = LSTMEmbeddingDecoder()(labels, state_trf, dt, rd)\n",
    "print(decoded.shape)\n",
    "\n",
    "predicted = CNNLSTMEmbeddingEncoderDecoder().generate((dt, rd, sr, un_sr, trf))\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "fa2d9f27-0642-43e0-a1ce-a9410d90accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = dataset_train[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "92776351-9407-4cf3-a341-1f2b834f0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_xs = torch.full_like(labels, -1)\n",
    "decoder_xs[..., 1:] = labels[..., :-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
